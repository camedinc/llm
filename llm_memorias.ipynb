{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Memorias\n",
    "\n",
    "Se presentan cuatro tipo de memorias que permiten entregar contexto al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token OpenIA\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conversation buffer\n",
    "\n",
    "Es el modelo más básico. Envía tanto el contenido recibido y el enviado para formar el contexto. El contenido aumenta rápido y agotará la cantidad de tokens disponibles rápidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "llm = ChatOpenAI()\n",
    "memoria = ConversationBufferMemory()\n",
    "chatbot = ConversationChain(llm = llm, memory = memoria, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¿Cómo me llamo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Juan.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interacción\n",
    "#chatbot.predict(input = \"¡Hola! ¿Cómo estás? Me llamo César y soy un ingeniero al que le gusta la ciencia ficción\")\n",
    "chatbot.predict(input = \"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='¿Cómo me llamo?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Tu nombre es Juan.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histórico de mensajes completo\n",
    "memoria.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversation buffer window\n",
    "Es similar a la anterior, pero enviará una pequeña ventana de los últimos mensajes que se han intercambiado. El k es el tamaño de la ventana de contexto que recuerda el modelo, a pesar que se le están enviando todas las interacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "llm = ChatOpenAI()\n",
    "memoria_buffer = ConversationBufferWindowMemory(k=3)\n",
    "chatbot_buffer = ConversationChain(llm = llm, memory = memoria_buffer, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¿Cómo me llamo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo digas. ¿Te gustaría decirme tu nombre?'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interacción\n",
    "#chatbot_buffer.predict(input = \"¡Hola! ¿Cómo estás? Me llamo César y soy un ingeniero al que le gusta la ciencia ficción\")\n",
    "#chatbot_buffer.predict(input = \"¿Cómo me llamo?\")\n",
    "#chatbot_buffer.predict(input = \"¿Cuánta altura tiene la Gran Pirámide de Egipto?\")\n",
    "#chatbot_buffer.predict(input = \"¿Cuál es la distancia a la Luna?\")\n",
    "#chatbot_buffer.predict(input = \"¿Cuál es la persona viva con mayor IQ del mundo?\")\n",
    "#chatbot_buffer.predict(input = \"¿Cuál es la persona en la historia con mayor IQ registrado?\")\n",
    "chatbot_buffer.predict(input = \"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='¿Cómo me llamo?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo digas. ¿Te gustaría decirme tu nombre?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histórico de mensajes\n",
    "memoria_buffer.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversation summary\n",
    "\n",
    "Las interacciones se envían a un segundo modelo que hace un resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "#llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# El modelo que resume es el mismo llm\n",
    "memoria_resumen = ConversationSummaryMemory(llm=llm)\n",
    "chatbot_resumen = ConversationChain(llm = llm, memory = memoria_resumen, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¿Cómo me llamo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No puedo saber tu nombre, ya que soy un programa de inteligencia artificial y no tengo acceso a esa información. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interacción\n",
    "#chatbot_resumen.predict(input = \"¡Hola! ¿Cómo estás? Me llamo César y soy un ingeniero al que le gusta la ciencia ficción\")\n",
    "#chatbot_resumen.predict(input = \"¿Cómo me llamo?\")\n",
    "#chatbot_resumen.predict(input = \"¿Cuánta altura tiene la Gran Pirámide de Egipto?\")\n",
    "#chatbot_resumen.predict(input = \"¿Cuál es la distancia a la Luna?\")\n",
    "#chatbot_resumen.predict(input = \"¿Cuál es la persona viva con mayor IQ del mundo?\")\n",
    "#chatbot_resumen.predict(input = \"¿Cuál es la persona en la historia con mayor IQ registrado?\")\n",
    "chatbot_resumen.predict(input = \"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='¿Cómo me llamo?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No puedo saber tu nombre, ya que soy un programa de inteligencia artificial y no tengo acceso a esa información. ¿Hay algo más en lo que pueda ayudarte?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histórico de mensajes\n",
    "memoria_resumen.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Knowledge graph memory\n",
    "\n",
    "Construye un contexto incremental a partir de las interacciones, basado en conceptos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "import networkx as nx\n",
    "\n",
    "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
    "#llm = ChatOpenAI()\n",
    "\n",
    "# El modelo que resume es el mismo llm\n",
    "memoria_kgm = ConversationKGMemory(llm=llm)\n",
    "chatbot_kgm = ConversationChain(llm = llm, memory = memoria_kgm, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ¿Cuánta altura tiene la Gran Pirámide de Egipto?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La Gran Pirámide de Giza tiene una altura original de 146.6 metros, pero debido a la pérdida de parte de la cubierta de piedra caliza, su altura actual es de alrededor de 138.8 metros.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interacción\n",
    "#chatbot_kgm.predict(input = \"¡Hola! ¿Cómo estás? Me llamo César y soy un ingeniero al que le gusta la ciencia ficción\")\n",
    "#chatbot_kgm.predict(input = \"¿Cómo me llamo?\")\n",
    "chatbot_kgm.predict(input = \"¿Cuánta altura tiene la Gran Pirámide de Egipto?\")\n",
    "#chatbot_kgm.predict(input = \"¿Cuál es la distancia a la Luna?\")\n",
    "#chatbot_kgm.predict(input = \"¿Cuál es la persona viva con mayor IQ del mundo?\")\n",
    "#chatbot_kgm.predict(input = \"¿Cuál es la persona en la historia con mayor IQ registrado?\")\n",
    "#chatbot_kgm.predict(input = \"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gran Pirámide de Egipto', '146.6 metros', 'tiene una altura de')]\n"
     ]
    }
   ],
   "source": [
    "print(chatbot_kgm.memory.kg.get_triples())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
